# Thoughts about the Future of Imaging Science 

_In his famous talk "You and Your Research," Richard Hamming described his practice of reserving a portion of his Fridays for what he called "Great Thoughts" time. During these sessions, he focused on significant, long-term ideas, primarily concerning the future of computing and Bell Labs. Inspired by Hamming, I implemented this practice in my routine to enhance my thinking. While my thoughts don’t compare to one of the GOATs, I found it fun. This blog post compiles some of my reflections from these sessions. As an aspiring Imaging Scientist, I concentrated mainly on the future of Imaging Science, though occasionally, other intriguing topics captured my attention._

## Intro

To predict the future of any field, it’s crucial first to identify its driving force. For example, predicting a boat’s trajectory involves understanding the direction of its motor and rudder. Similarly, predicting the future of a field requires analyzing its primary driver. In science and technology, this driver is the availability of cheap and powerful computation. Advances in simulation and design tools have made scientific experimentation easier, more efficient, and less costly. Moreover, the extensive data manipulations enabled by computational power have revolutionized research and the insights it generates. The lack of strong computation has been a main limiter in some of the more computation-intensive fields. The algorithms and data existed for many years, but the computational power was not. 

The continuous improvement of computation is largely due to Moore’s Law, which posits that the number of transistors on an integrated circuit doubles approximately every two years. Despite debates over the exact rate and specifics, it is clear that computational power is increasing rapidly. For instance, Jensen Huang from NVIDIA noted that GPU power has increased by 1000x in the last decade. This unprecedented growth in computational power, unmatched by any other asset or natural resource in human history, has significantly reshaped society. Technology plays an increasingly central role in our lives, driven by this relentless advance in computational capability.

***My grand thesis for the future centers on the eventual end of Moore’s Law and its profound impact on the field of science, particularly Imaging Science. When the rapid advancement of computational power slows, the consequences will be enormous, significantly altering the trajectory of scientific research. Imaging Science, which relies heavily on powerful computation for data analysis and visualization, will face substantial challenges and shifts in direction as a result.***

Claiming that Moore’s Law is dead has become somewhat cliché, with every semiconductor company attributing their poor quarterly numbers to some aspect of it. However, the reality is that the factors driving Moore’s Law are diminishing, and our current solutions to circumvent this are faltering. The easy gains in semiconductor physics and fabrication are over, leading to increasingly sporadic and slow progress.

Historically, the greatest driver of semiconductor performance gains was the ability to create smaller patterns through advanced lithography systems. By transitioning to smaller wavelengths, lithography systems allowed designs to shrink with minimal changes, providing almost free performance gains. However, this trend started to falter with the delayed introduction of extreme ultraviolet (EUV) systems. Despite eventually reaching production levels, EUV required significant design modifications, marking a departure from the past where simply reducing the wavelength sufficed.

This shift is evident with the new ASML lithography machines, which are now changing optical elements rather than wavelengths. Lowering the numerical aperture (NA) still meets Rayleigh's criterion for smaller dimensions but introduces greater complexity to semiconductor processes. This Faustian bargain has yet to convince skeptics of its long-term value. The latest high-NA system, which was supposed to restore Intel's dominance, is facing significant challenges, indicating that the tailwind driving computational power is weakening.

The exponential growth of GPUs and their computing power is often cited as a counterargument to this trend. However, this growth represents a clever workaround rather than a true solution. GPUs have optimized performance by lowering clock rates and increasing memory to enhance parallelization, an engineering tradeoff rather than a breakthrough in capability. Most of the gains are attributed to architectural improvements either in the form of numerical representations or sparsity in neural networks. This is a marked departure from the historical trend of improvement as a result of hardware improvements. While this approach has been highly successful, as seen with NVIDIA's impressive performance, it too will eventually hit a wall as further shrinking of chip dimensions becomes impossible. 

Another argument is that software improvements will compensate for hardware limitations. Yet, as Charles Leiserson's work suggests, while algorithmic improvements can yield substantial gains, they also encounter diminishing returns over time. Additionally, the shift from a CPU-dominated era to one with diverse custom chips will lead to uneven and sporadic improvements.

In summary, all the low-hanging fruit in semiconductor advancement has been picked along with most of the medium-hanging fruit, and as a result, the rate of progress in computation will inevitably slow. This impending slowdown presents significant challenges for fields dependent on continual computational advancements.

After discussing the waning power of computational advancements, I'll explain the potential impacts of this trend. The most obvious effect will be on Artificial Intelligence (AI) and Machine Learning (ML). These models are extremely computationally intensive for both training and inference. As computational power diminishes, the appeal and feasibility of these models will decrease. Already, the economic costs to train some foundation models are staggering, reaching into the billions, if not trillions. Additionally, powering data centers—both in terms of high-quality data and actual energy—poses significant challenges, alongside various intellectual property issues. As Bill Daly puts it, “Deep Learning was constrained by hardware, and will be constrained by hardware”.

This does not mean that AI will disappear. Models still outperform humans in certain tasks, particularly in computer vision, and will significantly benefit automation. However, the focus will likely shift to smaller, more efficient models rather than the massive ones currently striving for artificial general intelligence (AGI). This shift will necessitate a reevaluation of the economic and practical viability of large-scale AI models, emphasizing efficiency and practicality over sheer computational power.

Now, let's discuss how a slowdown in computational advancements will impact the field of Imaging Science. At its core, Imaging Science involves manipulating patterns of electromagnetic radiation to derive insights, blending optics, computation, solid-state and semiconductor physics, and psychophysics. One of the major drivers of this field is computation, so a slowdown in computational power will have significant effects.

Over the past few decades, Imaging Science has progressed due to advances in semiconductors and computational power. Developments in semiconductor physics have enabled scientists to capture and manipulate a broader range of the electromagnetic spectrum more efficiently, particularly with CMOS image sensors. Increased computational power has allowed for sophisticated processing and manipulation of the data collected, yielding higher-quality insights. Additionally, the simulation and design of imaging systems have benefited from these computational advancements.

However, if computational power starts to decline, the progress in Imaging Science will likely slow down. The ability to process complex imagery and run intricate algorithms will be hindered, affecting both the quality and efficiency of insights. Currently, I can identify several trends in Imaging Science that will likely be hindered by the decline in computational power. Recent advancements in deep learning techniques, such as neural radiance fields (NeRFs), computational imaging, and lensless imaging, are likely to face significant obstacles. As computational power decreases, experiments will take increasingly longer, and the cost of computation will rise, limiting the scope of applications.

Additionally, the burgeoning field of neuromorphic image sensors and other exotic sensing paradigms will be affected. These sensor modalities rely heavily on intense computational power and complex algorithms to derive insights. While algorithmic improvements could partially mitigate the impact, progress would be sporadic rather than the steady advancements previously enabled by hardware improvements.

In summary, the slowdown in computational power will create substantial challenges for these cutting-edge techniques and trends, potentially stalling their development and limiting their applications.

This raises the question of what’s next. I believe the future will consist of two main thrusts: one practical and the other more speculative. The practical push will focus on efficiency, maximizing the utility of our computational resources. In the past, the emphasis has been on enabling new capabilities without much regard for cost. This mindset will shift, with computational techniques undergoing intense scrutiny for cost-effectiveness. For instance, we might see approaches similar to Daly’s visual difference predictors and the concept of visual equivalence, identifying which computational elements offer the most significant perceptual and insightful benefits.

The speculative thrust involves redefining the role of imagery in Imaging Science. Traditionally, imagery has been treated as a data source from which insights are extracted. The next evolution may involve using imagery as a tool for thinking itself. Cognition doesn’t occur solely inside one’s head; it often involves external tools like pencils, paper, or software. Since perception is inherently a form of thinking, optimizing imagery and representations to enhance cognitive processes is a natural progression. This could lead to new ways of using images to accelerate understanding and innovation, transforming how we interact with and derive insights from visual data. Perhaps the word tool is a little weak, but rather imagery could be a new medium for thought borrowing from Alan Kay.

Since the future is uncertain, this is not a final prediction. The biggest challenge to this thesis would be if Moore’s Law continues unabated, with computing power advancing at its current pace. Historically, human ingenuity has often found solutions at the last moment, making this scenario quite possible. Even if computational progress continues steadily, I still believe that the future of Imaging Science will largely involve leveraging human cognitive systems. This could mean achieving greater efficiency or enhancing cognitive capabilities through advanced imaging techniques. The field's strong foundations and the potential for significant growth make it ripe for substantial innovation and expansion.
